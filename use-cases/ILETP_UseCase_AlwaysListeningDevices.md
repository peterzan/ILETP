<!-- SPDX-License-Identifier: CC-BY-4.0 -->
<!-- Copyright 2025 Peter Zan. Licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0). See LICENSE-CC-BY-4.0.txt in the repository root. -->

# Use Case: Always Listening Devices: Safety, Trust & Accountability
## Making AI a Practical and Safe Social Experience with ILETP™

The recent surge in AI-powered wearables — pendants, pins, glasses, earbuds, bracelets — signals a shift from AI as a tool we *invoke* to AI as a companion that is *always present*. Meta’s acquisition of Limitless, alongside competitors pursuing similar form factors, highlights a simple trend: **ambient, always listening AI devices are arriving faster than society is prepared for**.

These devices promise convenience, real-time memory capture, transcription, instant retrieval, continuous context awareness. But they also raise profound questions. Who controls recordings? Are conversations consensually captured? How long is data stored? Is processing on-device or cloud-routed? How do bystanders know they’re being recorded? When something goes wrong, how is it audited or remediated?

There is no widely accepted answer today. And that gap creates a moment where open-source standards can matter.

---

## Why this matters

Always-on wearables move AI from the screen into daily environments — workplaces, restaurants, classrooms, sidewalks, private homes. The leap isn’t technical as much as social. Adoption hinges less on model quality and more on **trust**, **privacy**, **consent visibility**, and **accountability during failure or misuse**.

While legislation may eventually address these areas, policy moves slowly, and early implementations risk fragmenting into vendor specific approaches that differ on data retention, consent signals, logging formats, and protection expectations. Without alignment, users may not trust the category, developers may hesitate to experiment, and backlash could delay progress for years.

This is where a community led effort could make a difference.

---

## How ILETP could be relevant (conceptually)

ILETP is **not** a wearable safety framework today.  But it could become a **foundation others build safety layers on top of**. Instead of dictating how devices must work, ILETP could provide a **neutral substrate for accountability**, inviting contribution from developers, ethicists, policy minds, and hardware innovators.

Examples of areas where community standards might emerge over time:

- Consent-by-design patterns for recording or transcription  
- Bystander awareness cues (visual, audio, physical indicators)  
- Expectations for data retention, local vs cloud processing  
- Guidance for incident reporting and misuse escalation  
- Logging formats that support external auditing  
- Optional profiles for privacy-sensitive environments  
- Community maintained “trust signals” for product UX  

These are **not feature commitments** — just **directions the community may explore** if interest grows.

The opportunity is not to define the standard outright, but to **seed a place where best practices can evolve in the open** rather than behind closed product walls.

---

## The risk of doing nothing

If each vendor crafts its own privacy model and consent mechanism, the result could be confusion, backlash, and inconsistent user protections. We’ve seen technology movements stall when public perception collapses, from Google Glass stigma to Humane AI Pin failures.

A fragmented start could slow adoption for everyone.

But if the developer community, early, neutral, open-source, begins exploring norms now, **trust can grow alongside capability instead of after crisis.** It gives regulators something concrete to reference, gives consumers clarity, and gives builders a map for responsible experimentation.

The goal isn't to control direction, but to keep the future open, safe, and interoperable.

---

## Why this fits within ILETP’s mission

As AI systems become more ambient:

- **Trust moves from abstract to personal.**  
- **Accountability moves from model output to lived environment.**  
- **Safety becomes visible or violable in public spaces.**

Wearable devices are a practical domain where **neutral, community-shaped trust mechanisms** could ripple outward into policy, product design, and social expectation. It is a clear example of where an open technical baseline could reduce fragmentation and support better outcomes for everyone involved.

ILETP provides the seed.  The community decides where it grows.

---

### Call for exploration

If contributors are interested, this use case could evolve into:

- Drafts of privacy/consent UX patterns  
- Logging + retention expectation proposals  
- A discussion thread or working group for wearables  
- A lightweight “Wearables Profile Concept”  
- Cross-model evaluation experiments for always-on systems  

Participation doesn’t require hardware expertise, just curiosity, dialogue, and ideas.

---

**Always-listening AI introduces new promise — and new responsibility.**  
The path forward should be built collaboratively, not reactively.

This use case exists to spark that conversation.
